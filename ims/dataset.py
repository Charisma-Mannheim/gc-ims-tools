from ims import Spectrum
import numpy as np
import os
from glob import glob
import h5py
from scipy.interpolate import interp1d
from sklearn.preprocessing import LabelEncoder

class Dataset:

    def __init__(self, data, name, files, samples, labels, _path):
        """
        DataSet class to coordinate many Spectrum instances
        as dask delayed objects with label and sample names available
        as attributes.
        Maps Spectrum methods to all spectra in DataSet and contains
        methods that require multiple spectra.

        Use one of the read_... methods as alternative constructor.

        Parameters
        ----------
        data : list
            lists instances of Spectrum

        name : str
            Uses the folder name if alternative constructor is used.

        files : list
            Lists file names of every file that
            was originally in the dataset.

        samples : list
            Lists sample names, one entry per spectrum.

        labels : list
            Lists label names, one entry per spectrum.
        """
        self.name = name
        self.data = data
        self.files = files
        self.samples = samples
        self.labels = labels
        self.preprocessing = []

    def __repr__(self):
        return f'Dataset: {self.name}, {len(self)} Spectra'

    def __getitem__(self, key):
        return self.data[key]

    def __len__(self):
        return len(self.data)

    def __iter__(self):
        return iter(self.data)

    @property
    def sample_indices(self):
        """
        Property method. Gives information about where each
        sample is in the dataset.

        Returns
        -------
        dict
            Sample names as keys,
            lists with indices of spectra as values
        """
        u_samples = np.unique(self.samples)
        indices = []
        for i in u_samples:
            index = np.where(np.array(self.samples) == i)
            indices.append(index)

        indices = [list(i[0]) for i in indices]
        indices = dict(zip(u_samples, indices))
        return indices

    @staticmethod
    def _measurements(path, subfolders):
        """
        Lists paths to every file in folder.
        Optionally generates label and sample names by splitting file paths.
        """
        if subfolders:
            files = []
            samples = []
            labels = []
            paths = glob(f'{path}/*/*/*.')
            name = os.path.split(path)[1]
            for filedir in paths:
                filedir = os.path.normpath(filedir)
                file_name = os.path.split(filedir)
                files.append(file_name)
                sample_name = path.split(os.sep)[-2]
                samples.append(sample_name)
                label = path.split(os.sep)[-3]
                labels.append(label)
        else:
            paths = [os.path.normpath(i) for i in glob(f'{path}/*')]
            name = os.path.split(path)[1]
            files = [os.path.split(i)[1] for i in paths]
            samples = []
            labels = []

        return (paths, name, files, samples, labels)

    @classmethod
    def read_mea(cls, path, subfolders=False):
        """
        Reads all GAS mea files in directory.

        If subfolders=True expects the following folder structure
        for each label and sample:

        Data
        |--> Group A
            |--> Sample A
                |--> file a
                |--> file b

        Labels are auto-generated from directory names.

        Parameters
        ----------
        path : str
            Directory with the data.

        subfolders : bool, optional
            Uses subdirectory names as labels,
            by default True

        Returns
        -------
        Dataset
        """
        paths, name, files, samples, labels = Dataset._measurements(
            path, subfolders
        )
        data = [Spectrum.read_mea(i, subfolders) for i in paths]
        return cls(data, name, files, samples, labels)

    @classmethod
    def read_hdf5(cls, path, subfolders=False):
        """
        Reads all hdf5 files generated by Spectrum.to_hdf5
        method in input directory.

        (Preferred over read_zip because it is much faster.)

        Parameters
        ----------
        path : str
            Directory with the data.

        Returns
        -------
        Dataset
            data, samples and labels attributes are not
            ordered but correctly associated.
        """
        paths, name, files, samples, labels = Dataset._measurements(
            path, subfolders
        )

        data = [Spectrum.read_hdf5(i) for i in paths]

        samples = []
        labels = []
        files = []
        for i in paths:
            with h5py.File(i, 'r') as f:
                samples.append(str(f.attrs['sample']))
                labels.append(str(f.attrs['label']))
                files.append(str(f.attrs['name']))

        return cls(data, name, files, samples, labels)

    def to_npy(self, folder_name):
        """
        Exports values of all spectra as npy file.
        Makes a target directory with a data folder
        and a npy file with label labels.

        Use the load_npy function from ml submodule to read
        the data.

        Parameters
        ----------
        folder_name : str
            Name of new target directory.
        """
        os.mkdir(folder_name)
        os.mkdir(f'{folder_name}/data')
        le = LabelEncoder()
        labels = le.fit_transform(self.labels)
        np.save(f'{folder_name}/labels.npy', labels)
        np.save(f'{folder_name}/label_names.npy', self.labels)
        np.save(f'{folder_name}/samples.npy', self.labels)
        exports = []
        for i, j in enumerate(self.data):
            exports.append(np.save(f'{folder_name}/data/{i}', j.values))

    def to_hdf5(self, folder_name):
        """
        Exports all spectra as hdf5 files.
        Saves them to new folder.

        Parameters
        ----------
        folder_name : str
            Name of new target directory.
        """
        os.mkdir(folder_name)
        [Spectrum.to_hdf5(i, path=folder_name) for i in self.data]

    def select(self, label=None, sample=None):
        """
        Selects all spectra of specified label or sample.
        Must give at least one argument.

        Parameters
        ----------
        label : str, optional
            Label name to keep, by default None

        sample : str, optional
            Sample name to keep, by default None

        Returns
        -------
        Dataset
            Contains only matching spectra.
        """
        if label is None and sample is None:
            raise ValueError("Must give either label or sample value.")
        
        if label is not None:
            name = label
            indices = []
            for i, j in enumerate(self.labels):
                if j == label:
                    indices.append(i)
        if sample is not None:
            name = sample
            indices = []
            for i, j in enumerate(self.samples):
                if j == sample:
                    indices.append(i)

        result = []
        files = []
        labels = []
        samples = []
        for i in indices:
            result.append(self.data[i])
            files.append(self.files[i])
            labels.append(self.labels[i])
            samples.append(self.samples[i])

        return Dataset(
            data=result,
            name=name,
            files=files,
            samples=samples,
            labels=labels,
        )

    # TODO: Write groupby method that returns multiple DataSet instances
    # def groupby(self):
    #     indices = self.sample_indices
    #     u_samples = np.unique(self.samples)

    #     g_labels = []
    #     g_samples = []
    #     g_files = []
    #     g_data = []
    #     for i in u_samples:
    #         idx = indices[i]
    #         labels = []
    #         samples = []
    #         files = []
    #         data = []
    #         for j in idx:
    #             labels.append(self.labels[j])
    #             samples.append(self.samples[j])
    #             files.append(self.files[j])
    #             data.append(self.data[j])

    #         g_labels.append(labels)
    #         g_samples.append(samples)
    #         g_files.append(files)
    #         g_data.append(data)

    #     self.labels = g_labels
    #     self.samples = g_samples
    #     self.files = g_files
    #     self.data = g_data
    #     return self

    def mean(self):
        """
        Calculates means for each sample,
        in case of repeat determinations.
        Automatically groups by sample.

        Returns
        -------
        Dataset
            With mean spectra.
        """
        indices = self.sample_indices
        u_samples = np.unique(self.samples)

        labels = []
        grouped_data = []
        for i in u_samples:
            label = self.labels[indices[i][0]]
            labels.append(label)

            data = []
            index = indices[i]
            for j in index:
                data.append(self.data[j])
            grouped_data.append(data)

        means = []
        for i in grouped_data:
            means.append(Spectrum.mean(i))

        self.data = means
        self.samples = list(u_samples)
        self.labels = labels
        self.preprocessing.append('mean')
        return self

    # TODO: rewrite method so it is no longer a massive bottleneck
    def riprel(self):
        """
        Interpolates all spectra to common RIP relative
        drift time coordinate.
        Alignment along drift time coordinate.

        Returns
        -------
        Dataset
            With RIP relative spectra.
        """
        dt_riprel = []
        interp_fn = []
        for i in self.data:
            dt = i.drift_time
            rip = np.median(np.argmax(i.values, axis=1)).astype('int32')
            rip_ms = np.mean(dt[rip])
            riprel = dt / rip_ms
            f = interp1d(riprel, i.values, axis=1, kind='cubic')
            dt_riprel.append(riprel)
            interp_fn.append(f)

        start = max([i[0] for i in dt_riprel])
        end = min([i[-1] for i in dt_riprel])
        interv = np.median([(i[-1]-i[0]) / len(i) for i in dt_riprel])
        new_dt = np.arange(start, end, interv)

        for i, f in zip(self.data, interp_fn):
            i.values[:, :len(new_dt)]
            i.values = f(new_dt)
            i.drift_time = new_dt
            i._drift_time_label = 'Drift Time RIP relative'

        return self

    def rip_scaling(self):
        """
        Scales values relative to global maximum for each spectrum.

        Returns
        -------
        Dataset
            With scaled values.
        """
        self.data = [Spectrum.rip_scaling(i) for i in self.data]
        self.preprocessing.append('rip_scaling')
        return self
    
    def resample(self, n):
        """
        Resamples spectrum by calculating means of every n rows.
        (Retention time coordinate needs to be divisible by n)

        Parameters
        ---------
        n : int

        Returns
        -------
        GCIMS-DataSet
            Resampled values array for each spectrum

        """       
        self.data = [Spectrum.resample(i, n) for i in self.data]
        self.preprocessing.append(f'resample({n})')
        return self

    def cut_dt(self, start, stop):
        """
        Cuts spectra on drift time coordinate.
        Specifiy coordinate values not index directly.

        Parameters
        ----------
        start : int/float
            start value on drift time coordinate
        
        stop : int/float
            stop value on drift time coordinate

        Returns
        -------
        Dataset
            With cut spectra.
        """
        self.data = [Spectrum.cut_dt(i, start, stop) for i in self.data]
        self.preprocessing.append(f'cut_dt({start, stop})')
        return self

    def cut_rt(self, start, stop):
        """
        Cuts spectra on retention time coordinate.
        Specifiy coordinate values not index directly.

        Parameters
        ----------
        start : int/float
            start value on retention time coordinate
        
        stop : int/float
            stop value on retention time coordinate

        Returns
        -------
        Dataset
            With cut spectra.
        """
        self.data = [Spectrum.cut_rt(i, start, stop) for i in self.data]
        self.preprocessing.append(f'cut_rt({start, stop})')
        return self

    def export_plots(self, folder_name, file_format='jpg', **kwargs):
        """
        Exports a static plot for each spectrum to disk.
        Replicates label folders.

        Parameters
        ----------
        folder_name : str, optional
            New directory to save the images.

        file_format : str, optional
            by default 'jpeg'
        """
        group_names = np.unique(self.labels)
        sample_names = np.unique(self.samples)
        sample_indices = self.sample_indices
        os.mkdir(folder_name)
        for label in group_names:
            os.mkdir(f'{folder_name}/{label}')

        for i in sample_names:
            indices = sample_indices[i]
            for j in indices:
                label = self.labels[j]
                Spectrum.export_plot(
                    self.data[j], path=f'{folder_name}/{label}',
                    file_format=file_format, **kwargs)

    def get_xy(self, flatten=True):
        """
        Returns X and y for machine learning as
        numpy arrays.

        Parameters
        ----------
        flatten : bool, optional
            Flattens 3D datasets to 2D,by default True

        Returns
        -------
        tuple
            (X, y) as np.arrays
        """
        X = [i.values for i in self.data]
        X = np.stack(X)
        y = np.array(self.labels)

        if flatten:
            a, b, c = X.shape
            X = X.reshape(a, b*c)

        return (X, y)
